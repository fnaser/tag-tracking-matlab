@inproceedings{BLUT,
abstract = {Visual tracking plays an important role in many computer vision tasks. A common assumption in previous methods is that the video frames are blur free. In reality, motion blurs are pervasive in the real videos. In this paper we present a novel BLUr-driven Tracker (BLUT) framework for tracking motion-blurred targets. BLUT actively uses the information from blurs without performing debluring. Specifically, we integrate the tracking problem with the motion-from-blur problem under a unified sparse approximation framework. We further use the motion information inferred by blurs to guide the sampling process in the particle filter based tracking. To evaluate our method, we have collected a large number of video sequences with significatcant motion blurs and compared BLUT with state-of-the-art trackers. Experimental results show that, while many previous methods are sensitive to motion blurs, BLUT can robustly and reliably track severely blurred targets.},
author = {Wu, Yi and Ling, Haibin and Yu, Jingyi and Li, Feng and Mei, Xue and Cheng, Erkang},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126357},
isbn = {9781457711015},
issn = {1550-5499},
pages = {1100--1107},
title = {{Blurred target tracking by blur-driven tracker}},
year = {2011}
}

@inproceedings{KalmanFilter,
  title={A quaternion-based unscented Kalman filter for orientation tracking},
  author={Kraft, Edgar},
  booktitle={Proceedings of the Sixth International Conference of Information Fusion},
  volume={1},
  pages={47--54},
  year={2003}
}

@misc{ParticleNotes,
author = {Orhan, Emin},
title = {{Particle Filtering}},
url = {http://www.cns.nyu.edu/{~}eorhan/notes/particle-filtering.pdf},
urldate = {2017-01-15},
year = {2012}
}


@article{AprilTag,
abstract = {While the use of naturally-occurring features is a central focus of machine perception, artificial features (fiducials) play an important role in creating controllable experiments, ground truthing, and in simplifying the development of systems where perception is not the central objective. We describe a new visual fiducial system that uses a 2D bar code style {\&}{\#}x201C;tag{\&}{\#}x201D;, allowing full 6 DOF localization of features from a single image. Our system improves upon previous systems, incorporating a fast and robust line detection system, a stronger digital coding system, and greater robustness to occlusion, warping, and lens distortion. While similar in concept to the ARTag system, our method is fully open and the algorithms are documented in detail.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Olson, Edwin},
doi = {10.1109/ICRA.2011.5979561},
eprint = {arXiv:1011.1669v3},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3400--3407},
pmid = {15991970},
title = {{AprilTag: A robust and flexible visual fiducial system}},
url = {http://april.eecs.umich.edu},
year = {2011}
}

@inproceedings{AprilTag_2,
  title={AprilTag 2: Efficient and robust fiducial detection},
  author={Wang, John and Olson, Edwin},
  booktitle={Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  pages={4193--4198},
  year={2016},
  organization={IEEE}
}

@article{BlurResilient,
author = {Prasad, Meghshyam G. and Chandran, Sharat and Brown, Michael S.},
doi = {10.1109/WACV.2015.41},
isbn = {9781479966820},
journal = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
pages = {254--261},
title = {{A motion blur resilient fiducial for quadcopter imaging}},
year = {2015}
}


@inproceedings{RUNETag,
  title={RUNE-Tag: A high accuracy fiducial marker with strong occlusion resilience},
  author={Bergamasco, Filippo and Albarelli, Andrea and Rodola, Emanuele and Torsello, Andrea},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on},
  pages={113--120},
  year={2011},
  organization={IEEE}
}

@inproceedings{AugmentedReality,
  title={A multi-ring color fiducial system and an intensity-invariant detection method for scalable fiducial-tracking augmented reality},
  author={Cho, Youngkwan and Lee, Jongweon and Neumann, Ulrich},
  booktitle={In IWAR},
  year={1998},
  organization={Citeseer}
}

@article{TagSLAM,
abstract = {Laser scanners have been proven to provide reli-able and highly precise environment perception for micro aerial vehicles (MAV). This oftentimes makes them the first choice for tasks like obstacle avoidance, close inspection of structures, self-localization, and mapping. However, artificial environments may pose problems if the scene is self-similar or symmetric and, hence, localization becomes ambiguous if only relying on distance measurements (e.g., when flying along a parallel aisle). In this paper, we propose to tackle these instances by introducing visual fiducial markers into the scene, detecting them with copter-mounted cameras and fusing these detections with laser-based self-localization in a graph optimization. Our approach abstracts the underlying multiple stages of laser-based SLAM to a slim interface that is only connected to the map building process and augments the self-localization in uncertain situations. We demonstrate the applicability of our approach during experiments in an indoor scenario with sparsely distributed fiducial markers. The test encompasses accurate map building with both the laser scanner and video cameras and subsequent relocalization relying on the detection of fiducial markers only.},
author = {Houben, Sebastian and Droeschel, David and Behnke, Sven},
doi = {10.0/Linux-x86_64},
keywords = {Air),Information Fusion,Mobile Robots (Land,SLAM,Sea},
title = {{Joint 3D Laser and Visual Fiducial Marker based SLAM for a Micro Aerial Vehicle}},
year = {2016}
}