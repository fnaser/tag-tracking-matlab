\documentclass[letterpaper, 10 pt, conference]{ieeeconf}

\makeatletter
\let\NAT@parse\undefined
\makeatother

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{pgfplots}

\usepackage{natbib}
\usepackage{hyperref}

\pgfplotsset{compat=1.7}

\graphicspath{ {figures/} }

\renewcommand{\vec}[1]{\boldsymbol{#1}}

\renewcommand{\eqref}[1]{\textup{{\normalfont(\ref{#1}}\normalfont)}}

\title{\textbf{AprilTrack: A Particle-Filter-based AprilTag Tracker}}
\author{Daniel Pfrommer}
\date{}

\begin{document}

\maketitle

\begin{abstract}

	The AprilTrack algorithm proposed in this paper bridges two existing fields of research in computer vision: the detection of fiducial markers (bar code type tags), and monocular object tracking. Visual tags are highly popular for establishing ground truth location in visual odometry experiments and on mobile robots, but detection can be difficult under rapid camera motion and in poor lighting conditions due to motion blur. In contrast with other works proposing redesigned, more blur resistant tags, we use the popular AprilTag fiducials and improve detection quality by means of a particle filter based monocular object tracker. Our algorithm is able to reliably track a fiducial's position and orientation in six degrees of freedom, allowing for tag-based localization in situations where current detectors fail. This novel patch-based particle filter tracking algorithm, which is extended to simultaneously track multiple tags in ego-motion scenarios, is qualitatively and quantitatively evaluated on several AprilTag sequences, demonstrating that it is capable of reliably localizing blurred tags.
		
\end{abstract}

\section{Introduction}


Visual tracking of fiducial markers (or simply fiducials) plays an important role in many robotics and computer vision related fields and applications, including augmented and virtual reality \citep{AugmentedReality}, visual odometry/SLAM \citep{TagSLAM}, and object tracking. However, as many of these applications involve the use of fiducials in structured indoors environments where illumination is often limited, long exposure times to compensate for the reduced lighting produce blurred images, especially when taken from a mobile platform such as a quadrotor or a handheld device.

The focus of this work is primarily on providing better pose estimation for visual odometry datasets which use AprilTags placed to known locations to provide the ground truth pose data. As the pose estimation in this scenario can be done in a post-processing step after the data has been collected, the current implementation of AprilTrack is not designed to run in realtime. We intend to explore the use of AprilTrack in realtime scenarios in future research.

\subsection{Previous Work}

 The majority of previous literature in fiducial detection has been focused on single-frame fiducial detection \citep{RUNETag, BlurResilient, AprilTag}. A common assumption in these algorithms is that the image being processed shows little motion blur, an expectation which breaks down in practice. For instance, the widely-used AprilTag marker \citep{AprilTag} and its successor, the AprilTag 2 \citep{AprilTag_2}, rely on strong gradients between the tag's background and the surrounding white border to detect the tag. As shown in figure \ref{fig:blurred_tag}, oftentimes such algorithms fail to detect tags that have been blurred, either missing them entirely or mistakenly discarding them when eliminating false positives through some encoded fiducial ID (for AprilTags this consists of a 6x6 binary grid in the center of the tag which is used to verify the validity of the detection).

\begin{figure}[b]
	\centering
	\includegraphics[width=6cm, height=4cm]{blurred_tag}
	\caption{A blurred AprilTag. Such tags are difficult for still frame based algorithms to detect as the payload (the ID encoded in the center of the tag) will cause the tag to be discarded as a false positive.}
	\label{fig:blurred_tag}
\end{figure}

Previously, there has been interest in designing blur resilient fiducials. Meghshyam et al. \citep{BlurResilient} have developed ringed markers to ensure that a region of the tag will always be perpendicular to the blur direction. However, this approach, while resistent to linear blur, does not take into account more complex blurs (e.g resulting from the tag pivoting around the vertical axis). In addition, the increased resilience to blur comes at the cost of localization accuracy as only the center point of the ringed tag can be reliably determined \citep{BlurResilient}, whereas with other fiducials, such as the AprilTag \citep{AprilTag}, full pose estimation in 6 degrees of freedom (DOF) can be done based on just a single tag. Another significant disadvantage of the ringed marker of Meghshyam et al. is the limited number of tag ID's, as their proposed encoding method allows for a maximum of 4 distinct tags \citep{BlurResilient}.


Unlike previous attempts to engineer more resilient tags, the algorithm proposed here exploits the temporal coherence between successive frames in a video sequence to track a tag's location between successful detections. As many high-blur scenarios involve a continuous stream of images taken at equidistant time intervals (e.g a quadroter-mounted moving camera viewing tags or a stationary camera tracking an AprilTag-labeled moving object), we propose to use knowledge of a fiducial's prior location to augment the detection algorithm. As such, this paper presents a novel particle-filter-based tracking algorithm (AprilTrack) capable of tracking AprilTags in 6 DOF with both joint tag motion and camera ego-motion.
	
Previous work involving the tracking of blurred objects includes the Blur-driven Tracker (BLUT) framework proposed by Wu et al. \citep{BLUT} for tracking motion-blurred targets. BLUT, a particle-filter based tracker, works on the observation that blur strength and direction can be used to estimate an object's velocity and provide valuable information for successfully tracking a blurred object. While capable of reliably tracking blurred objects, BLUT suffers from the disadvantage that it can do so only in two dimensions (i.e it is assumed that the object always faces the camera) and so cannot track a sequence involving arbitrary tag rotations and translations.


The novel method proposed in this paper augments the AprilTag detection algorithm originally designed by Olson \citep{AprilTag} and improved by Wang et al \citep{AprilTag_2} with a particle-filter based algorithm to reliably track an AprilTag through an arbitrary sequence of images, providing fiducial-based position information when existing methods fail. In addition, the proposed algorithm is extended to incorporate the joint motion of an arbitrary number of tags to increase the accuracy of the tracked orientation and position.


\textbf{Overview.} The remainder of the paper is organized as follows. Section 2 gives an overview of the method proposed in this paper and describes the algorithm in detail. Section 3 describes the implementation of the algorithm and evaluates its performance on a manually-labeled dataset of blurred moving tags both quantitatively and qualitatively. In section 4 we discuss the limitations of this approach and suggest possible avenues of future research.

\section{Method}

The AprilTrack algorithm is similar in concept to the BLUT tracker by Wu et al. \citep{BLUT} in that it uses a particle filter to integrate all available information about a marker's position. Overall, the description of the algorithm can be broken down into three parts: one describing the particle filter, another on the method for evaluating candidate particles, and one describing the modifications made to simulatenously track multiple tags.

\subsection{The Particle Filter}

The particle filter used for the AprilTrack algorithm is a Bayesian Sequential Importance Resampling (SIR) technique which approximates a posterior distribution of state variables at discrete time intervals. There are three major steps in the algorithm: prediction, update, and resampling. We use the vector  $\vec{x}_t$  to denote the 13 dimensional state vector describing the position, orientation, velocity, and rotational velocity of the tracked target at time $t$ 

\begin{equation}
\vec{x}_t = 
\begin{bmatrix}
	\vec{r} \\
	\dot{\vec{r}} \\
	\vec{q} \\
	\vec{\omega}
\end{bmatrix}
\end{equation}

where $\vec{r}$ is the three dimensional position of the tracked object, $\dot{\vec{r}}$ is the three dimensional velocity, $\vec{q}$ is a four dimensional unit quaternion containing the pose of the tracked object, and $\vec{\omega}$ is an angular velocity with components $\omega_x$, $\omega_y$, and $\omega_z$.

\textbf{Optimal filtering.} The prediction step can be formulated in a Bayesian setting as the computation of the prior distribution $p(\vec{x}_{t}|\vec{z}_{t-1})$ from the filtering distribution $p(\vec{x}_{t-1}|\vec{z}_{1:t-1})$ where $\vec{z}_t$ represents the measurement (appearance of a fiducial) taken at time $t$. As in \citep{ParticleNotes}:

\begin{equation} \label{eq:predict}
p(\vec{x}_t|\vec{z}_{1:t-1}) = \int p(\vec{x}_t | \vec{x}_{t-1})p(\vec{x}_{t-1}|\vec{z}_{1:t-1})d\vec{x}_{t-1}
\end{equation}

Likewise, the update step can be formulated in a similar manner, where the prior distribution calculated in the prediction step is updated with the latest measurement to obtain the posterior over $\vec{x}_t$. Mathematically this can be derived using Bayes' law to express $p(\vec{x}_t|\vec{z}_{1:t})$ in terms of $p(\vec{z}_t|\vec{x}_t)$ and $p(\vec{x}_t|\vec{z}_{1:t-1})$. Using Bayes' law yields
\begin{equation*}
	p(\vec{x}_t|\vec{z}_{1:t}) = p(\vec{x}_t|\vec{z}_t,\vec{z}_{1:t-1})
\end{equation*}
\begin{equation*}
p(\vec{x}_t|\vec{z}_t,\vec{z}_{1:t-1}) = \frac{p(\vec{z}_t|\vec{x}_t, \vec{z}_{1:t-1})p(\vec{x}_t|\vec{z}_{1:t-1})p(\vec{z}_{1:t-1})}{p(\vec{z}_{1:t})}
\end{equation*}
As the model $p(\vec{x}_t|\vec{x}_{t-1})$ is Markovian, the current measurement is conditionally independent of previous measurements given $\vec{x}_t$, meaning that $p(\vec{z}_{t}|\vec{x}_t, \vec{z}_{1:t-1})$ can be written as $p(\vec{z}_t|\vec{x}_t)$. $p(\vec{z}_{1:t})$ can also be expressed as $p(\vec{z}_t|\vec{z}_{1:t-1})p(\vec{z}_{1:t-1})$ resulting in the final update equation \citep{ParticleNotes}
\begin{equation} \label{eq:update}
	p(\vec{x}_t|\vec{z}_{1:t}) = \frac{p(\vec{z}_{t}|\vec{x}_t)p(\vec{x}_{t}|\vec{z}_{1:t-1})}{p(\vec{z}_t|\vec{z}_{1:t-1})}
\end{equation}


\textbf{Particle Filters.}\footnote{A more in-depth discussion of particle filters can found in \citep{ParticleNotes}. Much of the notation used in \citep{ParticleNotes} is similar to that used here.} Particle filters attempt to approximate the posterior distribution at time $t-1$, $p(\vec{x}_{0:t-1}|\vec{z}_{1:t-1})$, using Monte-Carlo methods. A particle filter consists of a set of $N$ samples $\{ (\vec{x}^i_{t-1}, w^i_{t-1}) \}_{i=1}^N$ drawn from the proposal distribution $q(\vec{x}^i)$ and weighted so as to take into account any discrepancy between $q(\vec{x})$ and $p(\vec{x})$ in that $w^i_t \propto p(\vec{x}^i_{0:t}|\vec{z}_{1:t})/q(\vec{x}^i_{0:t}|\vec{z}_{1:t})$.

As with the optimal filtering scenario, the prior distribution (the distribution of particles at the current timestep before taking into account the measurement) is calculated in the propagation step. In a particle filter, this simply amounts to drawing a new set of particles based using

\begin{equation} \label{eq:propagate_particles}
	\vec{x}^i_t \sim q(\vec{x}_t|\vec{x}^i_{t-1}, \vec{z}_t)
\end{equation}

where $\sim$ denotes sampling from the state transition distribution $q(\vec{x}_t|\vec{x}^i_{t-1}, \vec{z}_t)$.

Given the prior distribution, the particle weights must then be updated to reflect the target distribution $p(x)$. As previously noted:
\begin{equation} \label{eq:weight_value}
	w^i_{t} \propto \frac{p(\vec{x}^i_{0:t}|\vec{z}_{1:t})}{q(\vec{x}^i_{0:t}|\vec{z}_{1:t}) }
\end{equation}
We assume the proposal distribution $q(\vec{x})$ can be factored as
\begin{equation*}
	q(\vec{x}^i_{0:t}|\vec{z}_{1:t}) = q(\vec{x}^i_t|\vec{x}_{0:t-1})q(\vec{x}^i_{0:t-1}|\vec{z}_{1:t-1})
\end{equation*}
Since $p(\vec{x}^i_{0:t}|\vec{z}_{1:t})$ can be rewritten (in a similar manner to the derivation of \eqref{eq:update} with Bayes' law) as
\begin{equation*}
	p(\vec{x}^i_{0:t}|\vec{z}_{1:t}) = 
	\frac{p(\vec{z}_t|\vec{x}^i_t)p(\vec{x}_t|\vec{x}^i_{0:t-1}, \vec{z}_{1:t-1})p(\vec{x}^i_{0:t-1}|\vec{z}_{1:t-1})}{p(\vec{z}_t|\vec{z}_{1:t-1})}
\end{equation*}
the equation from \eqref{eq:weight_value} can then be rewritten recursively in terms of $w^i_{t-1}$ with

\begin{equation} \label{eq:update_weights}
	w^i_t \propto w^i_{t-1} \frac{p(\vec{z}_t|\vec{x}^i_t)p(\vec{x}^i_t|\vec{x}^i_{t-1})}{q(\vec{x}^i_t|\vec{x}^i_{t-1},\vec{z}_t)}
\end{equation}

This allows us update the weights from the previous iteration and model the posterior distribution $p(\vec{x}_{0:t}|\vec{z}_{1:t})$ with the set of weighted particles. The highest weighted particle is seleted as the most likely candidate.

To prevent the set particles from degenerating over time into a handful of highly weighted particles, resampling is applied. Resampling consists of drawing a new set of $N$ particles from the discrete approximation of the filtering distribution $p(\vec{x}_t|\vec{z}_{1:t})$ with the set of weighted particles:

\begin{equation}
	p(\vec{x}_t|\vec{z}_{1:t}) \approx \sum_{i=1}^{N}{w^i_t\delta_{\vec{x}^i_t}}
\end{equation}
where the weights of the new resampled particles are then set to $1/N$.

The algorithm described and derived above is the Sequential Importance Sampling (SIS) filter. The AprilTrack tracker uses a Sequential Importance Resampling (SIR) filter, which deviates from the SIS algorithm in that resampling is applied after every iteration and the proposal distribution $q(\vec{x}_t|\vec{x}^i_{t-1})$ is simply the state transition distribution $p(\vec{x}_t|\vec{x}^i_{t-1})$. This means that the propagate and update equations from \eqref{eq:propagate_particles} and \eqref{eq:update_weights} simply become:

\begin{equation}
	\vec{x}^i_t \sim p(\vec{x}_t|\vec{x}^i_{t-1})
\end{equation}
and (with the weights normalized to a total of 1):
\begin{equation}
	w \propto p(\vec{z}_t|\vec{x}^i_t)
\end{equation}

In AprilTrack, we model $p(\vec{x}_t|\vec{x}^i_{t-1})$ as a multivariate gaussian distribution with standard deviations $\vec{\sigma}$ around a mean $\vec{\mu}$ where
\begin{equation} \label{eq:propagation_mean}
\vec{\mu} = 
\begin{bmatrix} 
	\vec{r}^i_{t-1} + \Delta t \dot{\vec{r}}^i_{t-1} \\
 	\dot{\vec{r}}^i_{t-1} \\
	\vec{q}^i_{t-1} \dot{\vec{q}}^i_{t-1}\\
	\vec{\omega}^i_{t-1}
\end{bmatrix}
\end{equation}
and where $\Delta t$ is the elapsed time and $\dot{\vec{q}}^i_{t-1}$ is the quaternion representation of $\Delta t  \vec{\omega}^i_{t-1}$, as described in \citep{KalmanFilter}. We also use the \emph{noise quaternion} formulated in \citep{KalmanFilter} to apply the gaussian noise to the quaternion $\vec{q}^i_{t-1} \dot{\vec{q}}^i_{t-1}$. For simplicity, we make the assumption that the input images have been taken at equidistant time intervals and incorporate $\Delta t$ into the $\vec{\omega}$ and $\dot{\vec{r}}$ components of $\vec{\sigma}$.

\subsection{Evaluation of Candidate Particles} \label{sec:candidate_particles}

The tracking of AprilTags in 6 DOF requires a method for evaluating the likelihood of candidate particles with arbitrary three dimensional positions and rotations in order to calculate $p(\vec{z}_t|\vec{x}^i_t)$. To this effect, the proposed method uses homographies to generate image \emph{patches}, which are unprojected representations of an AprilTag for a particular pose and image.


Given the function $I(\begin{bmatrix} \lambda x, & \lambda y, & \lambda \end{bmatrix}^\intercal )$ representing the intensity of pixel at a particular $(x,y)$ of an image, a patch in the image can be defined by the function 

\begin{equation}
P(x_p,y_p| \vec{H}, I ) = I(\vec{H}  \begin{bmatrix} x_p & y_p & 1\end{bmatrix}^\intercal)
\end{equation}

where $x_p$ and $y_p$ are between $-1$ and $1$ and $\vec{H}$ is a 3x3 matrix (a homography) mapping from $(x_p, y_p, 1)$ to $(\lambda x, \lambda y, \lambda)$ which incorporates the pose of an AprilTag relative to the camera. 


This allows for the evaluation of candidate particle $\vec{x}^i$ by the measuring the similarity between the particle's patch $P^i(x_p,y_p|\vec{H}^i,I)$ in the current image and the patch from the AprilTag's last successful detection, $P_{ref}(x_p,y_p|\vec{H}_{ref},I_{ref})$. Here, $H^i$ can be calculated with

\begin{equation} \label{eq:particle_homography}
\vec{H}^i = \begin{bmatrix}
		\widehat{\vec{q}^i} & \vec{r}^i
	\end{bmatrix} \begin{bmatrix}
		\tau/2 & 0 & 0 \\
		0 & \tau/2 & 0 \\
		0 & 0 & 0 \\
		0 & 0 & 1
	\end{bmatrix}
\end{equation}



where $\widehat{\vec{q}^i}$ is the rotation matrix representation of particle $\vec{x}^i$'s rotation quaternion $\vec{q}$, $\vec{r}$ is the particle's translation, and $\tau$ is the tag's edge length. The patch for the detected AprilTag uses the homography $\vec{H}_{ref}$ supplied by the AprilTag library, which computes the homography using the Direct Linear Transform algorithm \citep{AprilTag}. 


To compare the patches efficiently, $P^i$ and $P_{ref}$ are evaluated at a regular grid from $(-1, -1)$ to $(1, 1)$ at intervals of $2 / \rho$. This results in two $\rho \times \rho$ matrices $\vec{M}^i$ and $\vec{M}_{ref}$. To compare the patches, the error between the two  matrices is calculated using the correlation coefficient between the individual elements of the matrices.

\begin{equation}
	\epsilon(\vec{A}, \vec{B}) = \frac{1}{2} - \frac{\sum_{i=1}^{\rho} \sum_{j=1}^{\rho} (\vec{A}_{ij} - \mu_{\vec{A}})(\vec{B}_{ij} - \mu_{\vec{B}}) }{2\sigma_{A} \sigma_{B}}
\end{equation}

where $\vec{A}$ and $\vec{B}$ are the matrix approximations of two patches, $\sigma_{\vec{A}}$ and $\sigma_{\vec{B}}$ are the standard deviations of the respective matrices' elements and $\mu_{\vec{A}}$ and $\mu_{\vec{B}}$ are their means. By normalizing by mean and standard deviation, using correlation as a similarity metric (as opposed to the sum of the absolute difference or squared difference) ensures lighting invariance for comparing patches containing the same marker under different illuminations.


As in BLUT \citep{BLUT}, the patch error $\epsilon(\vec{M}^{i}, \vec{M}_{ref})$ is used to compute the observation likelihood $p(\vec{z}_t|\vec{x}^i_t)$ to update the weights and select the best particle with


\begin{equation}
	p(\vec{z}_t|\vec{x}^i_t) = exp(-\gamma \epsilon(\vec{M}^i, \vec{M}_{ref}))
\end{equation}

where the parameter $\gamma$ allows for more or less selective resampling.


\begin{figure}[b]
	\centering
	\includegraphics[width=7.5cm,height=5cm]{tracked_tag}
	\caption{A blurred tag being tracked. The red border indicates that the tag was not detected with the AprilTag detector.}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=7cm,height=4.75cm]{RefPatches}
	\caption{The reference patches of various tags (by ID) in a multi-Tag tracking scenario.}
\end{figure}

\begin{figure*}[t]
	\centering
	\label{fig:error}
	\begin{tikzpicture}[trim axis left]
	\begin{axis}[
		width=\textwidth,
		height=9cm,
		title= {Average Corner Error on the Single-Tag Sequence},
		xlabel = {Time (Frame Number)},
		ylabel = {Average Corner Error (Pixels)},
		xmin=0, xmax=250,
		ymin=0, ymax=80,
		xtick={0,50,100,150,200,250},
		ytick={0,10,20,30,40,50,60, 70}
	]
	\addplot [
		color=blue,
		mark=none,
	] table {corner_error.dat};
\end{axis}
\end{tikzpicture}
	\caption{The average corner error between the hand-marked tag location and the tracked location. Much of the error comes from discrepancy between the tracker and the ground truth on the boundaries of the blurred tags. At around frames 30 and 200, the tracker loses the tag momentarily due to a sudden rotation but is able to recover without needing to be manually reset.}
\end{figure*}




\subsection{Multi-Tag Scenarios}

To adapt this method to multi-tag scenarios, several extra parameters have to be introduced. In order to jointly track multiple tags, it is assumed that the relative poses of the tags are known. For each tag $T_k$ with ID $k$, the location $\vec{x}_{k}$ and orientation quaternion $\vec{g}_{k}$ are expressed such that

\begin{equation}
	\vec{X}_c = \widehat{\vec{q}^i} (\widehat{\vec{g}_k} \vec{X}_t + \vec{x}_k) + \vec{r}^i
\end{equation}
where $\vec{X}_t$ is a coordinate relative to the tag's reference frame, $\vec{X}_c$ is relative to the camera reference frame, and $\vec{q}^i$ and $\vec{r}^i$ are the position and orientation of particle $\vec{x}^i_t$. In the simple case of jointly moving tags, the $\vec{H}^i$ from equation \eqref{eq:particle_homography} can then be rewritten as


\begin{equation}
	\vec{H}^i_{k} =
	\begin{bmatrix}
		\widehat{\vec{q}^i \vec{g_k}} & \widehat{\vec{q}} \vec{x_k} + \vec{r}^i
	\end{bmatrix}\begin{bmatrix}
		\tau_k/2 & 0 & 0 \\
		0 & \tau_k/2 & 0 \\
		0 & 0 & 0 \\
		0 & 0 & 1
	\end{bmatrix}
\end{equation}

where $\vec{H}^i_k$ represents the homography for particle $i$ and tag $k$ and $\tau_k$ is the edge length of tag $k$.


In the ego-motion scenario, each particle's rotation and translation can be taken to be the camera's rotation and  translation such that

\begin{equation}
	\widehat{\vec{q}^i}\vec{X}_c + \vec{r}^i = \widehat{\vec{g}_k} \vec{X}_t + \vec{x}_k
\end{equation}

resulting in the homography $\vec{H}^i_k$ and the corresponding patch approximation $\vec{M}^i_k$ where

\begin{equation}
	\vec{H}^i_k = \begin{bmatrix}
		\widehat{\vec{q}^i}^{-1} \widehat{\vec{g}_k} & \widehat{\vec{q}^i}^{-1} (\vec{x}_k - \vec{r}^i)
	\end{bmatrix} \begin{bmatrix}
		\tau_k/2 & 0 & 0 \\
		0 & \tau_k/2 & 0 \\
		0 & 0 & 0 \\
		0 & 0 & 1
	\end{bmatrix}
\end{equation}


In both ego-motion and joint tag motion, the observation likelihood $p(\vec{z}_t|\vec{x}_{t})$ then becomes

\begin{equation}
	p(\vec{z}_t|\vec{x}^i_{t}) = exp(\sum^{K}_{k=1} -\gamma \epsilon(\vec{M}^i_k, \vec{M}_{k, ref}))
\end{equation}

where $K$ is the number of tags being tracked and $\vec{M}_{k,ref}$ is the reference patch approximation for tag $k$ from the last successful detection.


\begin{figure}
	\centering
	\includegraphics[width=8cm,height=6cm]{Particles2}
	\caption{The distribution of particles and their weights with respect to the X,Y position of the particles.}
\end{figure}

\section{Results and Discussion}

The AprilTrack algorithm was primarily evluated on a manually-labeled single-tag sequence with a moving tag. The algorithm was also run on an ego-motion multi-tag sequence which will be qualitatively discussed.
\footnote{Videos of the sequences and the tracker's performance are available online at https://www.youtube.com/watch?v=kV8z3st9rlk and https://www.youtube.com/watch?v=F5cFTLBlJ1s }


The specific implementation of the AprilTrack algorithm used in this experiment was written in MATLAB. For both scenarios, the parameters $\gamma = 10$, $\rho=32$ were used. $\tau$ was $0.1936$ m for all tags, slightly larger than the actual tag size of $0.1635$ m to include some of the whitespace around the tag. In the single-tag scenario, the standard deviation $\vec{\sigma}$  of the gaussian noise used in the state transition distribution $p(\vec{x}_t|\vec{x}^i_{t-1})$ was $0.01$ for the $\vec{r}$ component, $0.05$ for $\vec{q}$, $0.02$ for $\dot{\vec{r}}$, and $0$ for $\omega$.


When evaluated on the single-tag sequence, the algorithm ran with $N=3000$ at around $1$ fps on an i7 Haswell Intel CPU. In experiments with fewer particles, it was found that the algorithm could track a tag well with as few as $1000$ particles with around $5$ fps. With a more optimized C++ implementation, the framerate could perhaps be brought up to $10-15$ fps, making the algorithm (run with less agressive parameters) a possible candidate for real-time scenarios. Currently, the MATLAB implementation is intended for offline post-processing of a sequence and so real-time performance was not a focus of this research.


As shown in figure \ref{fig:error}, the pixel corner error is about 10 pixels for most of the sequence. The tracker loses the tag momentarily around frames 30 and 200 but is able to recover rapidly without any manual intervention. Much of the error observed throughout the sequence is the result of disagreement on the boundary of a blurred tag rather than a systematic issue with the tracker itself.

When running with a multi-tag configuration, the tracker is able to make use of the additional information to more accurately track the camera's location than in a single-tag scenario. The main challenge for running the algorithm with multiple tags was not successfully tracking the tags---the algorithm was able to do that with half the particles of the single-tag scenario. Rather, the majority of the reprojection error arises from error in the relative tag positions. This opens up new potential avenues of research, such as tag-based bundle adjustment, to alleviate these issues.

\section{Conclusion}

We have presented the AprilTrack algorithm, which augments standard AprilTag detection algorithms with a tracker capable of tracking fiducials in 6 DOF. Experimental results show that the tracker can track both single tags and consolidating information from multiple tags in ego-motion and joint tag-motion scenarios. In future research we plan to investigate other possible error metrics for patch comparison and look to deploy AprilTrack in realtime scenarios.

\bibliographystyle{abbrv}
\bibliography{paper_bib}{}

\end{document}
